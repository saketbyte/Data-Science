{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics\n",
    "### 1. Tokenisation\n",
    "\n",
    "### 2. Lexicographic corrections\n",
    "#### a. Lemmatization b. Stemming\n",
    "\n",
    "### 3. Bag of words and TF-IDF\n",
    "\n",
    "### 4. Word Embeddings:\n",
    "#### a. Word2Vec b. GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "     Tokenisation simply means to create units of words/sentences as per the needs. This is something very basic which is needed in order to create a vocabulary for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in c:\\users\\samriddh\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib in c:\\users\\samriddh\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: regex in c:\\users\\samriddh\\anaconda3\\lib\\site-packages (from nltk) (2020.2.20)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in c:\\users\\samriddh\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied, skipping upgrade: click in c:\\users\\samriddh\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk --upgrade\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens of sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "#Tokens of words from the para\n",
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words) #we have 399 words in the words variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences) #we have 31 sentences in the words variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicographic corrections:\n",
    "The step converts all the disparities of a word into their normalized form (also known as lemma).\n",
    "The most common lexicon normalization practices are :\n",
    "#### Stemming: \n",
    "    Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word. We are just finding the base word. This is used for analytical application.\n",
    "    Eg: playing, player, playboy, playstation, playground --> play\n",
    "        Finalists,Final, Finalle, Finance --> fina\n",
    "\n",
    "The problem is that the words after stemming may not have any meaning to us. Lemmatization takes care of maintaining a congruent nature with our human vocabulary to convey a sensible meaning.\n",
    "#### Lemmatization:\n",
    "    Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word. This is generally used where human interaction is needed.\n",
    "   **It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).**\n",
    "   \n",
    "    The outcome is a reduction in features to store only useful ones to for the model.\n",
    "    \n",
    "#### Stopwords\n",
    "Stopwords are those words which do not contribute anything significant to the model. Example-  The, this, is, of, in , from, and, our bla bla. A general characteristic is they are repeated multiple times.\n",
    "\n",
    "Remember that: Sometimes you would not wanna remove some helping verbs and other kind of words where tense of the sentences is to be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the list of stopwords in English. You can check for other languages.\n",
    "stopwords.words(\"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    #List Comprehension- if the word belongs to stopwords list then remove it else stem it.\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words(\"English\"))]\n",
    "    sentences[i]= \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No stop words and no extended words. See histori, peopl, invad etc as stemming.\n",
    "sentences\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "# List comprehension using Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = \" \".join(words)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary creation\n",
    "\n",
    "#### (1) Bag of words model\n",
    "###### a. Binary\n",
    "###### b. Non-Binary(Stores the frequency as well.)\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It provides with a vocabulary of known words and their frequency.\n",
    "Because we know the vocabulary has n words, we can use a fixed-length document representation of n, with one position in the vector to score each word. The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present ( one-hot-encoding on the data to formulate our vector x as binary or non-binary models ).The vectors x are derived from textual data, in order to reflect various linguistic properties of the text.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* This creates a very messy vector when the vocabulary is huge.\n",
    "    Solution: Use Word2Vec\n",
    "* The semantic of the words are not preserved. \"He is an intelligent boy.\" We would have same importance of the adjective and the noun here, hence can not derive much information.\n",
    "    Solution: Use TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text preprocessing before the BoW\n",
    "\n",
    "import re #regular exploration library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\"\n",
    "paragraph = paragraph.lower()\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    #split creates a list of words\n",
    "    review = review.split()\n",
    "    # list comprehension using stemming\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words(\"english\"))]\n",
    "    review = \" \".join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 114)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) TF-IDF (Term Frequency and Inverse Document Frequency)\n",
    "\n",
    "\n",
    "\n",
    "**TF = Number of repition of words in sentence[i] / Total words in sentence[i]**\n",
    "\n",
    "\n",
    "**IDF = log(Total number of sentences(n)/Number of sentences containing the word)**\n",
    "\n",
    "Examples:\n",
    "\n",
    "Sentence[0] = good boy\n",
    "\n",
    "Sentence[1] = good girl\n",
    "\n",
    "Sentence[2] = boy girl good \n",
    "\n",
    "**TF**\n",
    "\n",
    "| words | Sentence[0] | Sentence[1] | Sentence[2] |\n",
    "|-------|-------------|-------------|-------------|\n",
    "| good  | 1/2         | 1/2         | 1/3         |\n",
    "| boy   | 1/2         | 0           | 1/3         |\n",
    "| girl  | 0           | 1/2         | 1/3         |\n",
    "\n",
    "**IDF**\n",
    "\n",
    "| Words | IDF <br>log() |\n",
    "|-------|---------------|\n",
    "| good  | 3/3           |\n",
    "| boy   | 3/2           |\n",
    "| girl  | 3/2           |\n",
    "\n",
    "\n",
    "Now let us see the magic of analysing texts with mathematics: \n",
    "\n",
    "**TF x IDF**\n",
    "\n",
    "Basically, we product the two tables by multiplying TF of words with IDF of words in each sentence.\n",
    "\n",
    "| sentence[i] | f1 (good) | f2 (boy)       | f3 (girl)      |\n",
    "|-------------|-----------|----------------|----------------|\n",
    "| sentence[0] | <br>0     | 1/2 * log(3/2) | 0              |\n",
    "| sentence[1] | 0         | 0              | 1/2 * log(3/2) |\n",
    "| sentence[2] | 0         | 0              | 0              |\n",
    "\n",
    "We will have a label which will be a dependent feature of the above three features. For a larger vocabulary this works wonders as it emphasies on the presence of word being present in a sentence.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "We apply some mathematical formulae, in case of BoW and TF-IDF as well, but these causes some problems:\n",
    "1. Semantic information is not stored well in the two models.\n",
    "2. Highly ineffecient form of word representation and feature generation, as a lot of data present is a sparse matrix of the size of vocabulary.\n",
    "Say you have a dictionary of 10k words and Car is at 2500th position, we will have a vector of 10k-1 zeros and 1 one. We can not relate words to preserve semantic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular exploration library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\"\n",
    "\n",
    "paragraph = paragraph.lower()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    #split creates a list of words\n",
    "    review = review.split()\n",
    "    # list comprehension using stemming\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words(\"english\"))]\n",
    "    review = \" \".join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india',\n",
       " 'year history people world come invaded u captured land conquered mind',\n",
       " 'alexander onwards greek turk mogul portuguese british french dutch came looted u took',\n",
       " 'yet done nation',\n",
       " 'conquered anyone',\n",
       " 'grabbed land culture history tried enforce way life',\n",
       " '',\n",
       " 'respect freedom others first vision freedom',\n",
       " 'believe india got first vision started war independence',\n",
       " 'freedom must protect nurture build',\n",
       " 'free one respect u',\n",
       " 'second vision india development',\n",
       " 'fifty year developing nation',\n",
       " 'time see developed nation',\n",
       " 'among top nation world term gdp',\n",
       " 'percent growth rate area',\n",
       " 'poverty level falling',\n",
       " 'achievement globally recognised today',\n",
       " 'yet lack self confidence see developed nation self reliant self assured',\n",
       " 'incorrect',\n",
       " 'third vision',\n",
       " 'india must stand world',\n",
       " 'believe unless india stand world one respect u',\n",
       " 'strength respect strength',\n",
       " 'must strong military power also economic power',\n",
       " 'must go hand hand',\n",
       " 'good fortune worked three great mind',\n",
       " 'dr vikram sarabhai dept',\n",
       " 'space professor satish dhawan succeeded dr brahm prakash father nuclear material',\n",
       " 'lucky worked three closely consider great opportunity life',\n",
       " 'see four milestone career']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['i have three visions for india.',\n",
       " 'in 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
       " 'from alexander onwards, the greeks, the turks, the moguls, the portuguese, the british,\\n               the french, the dutch, all of them came and looted us, took over what was ours.',\n",
       " 'yet we have not done this to any other nation.',\n",
       " 'we have not conquered anyone.',\n",
       " 'we have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
       " 'why?',\n",
       " 'because we respect the freedom of others.that is why my \\n               first vision is that of freedom.',\n",
       " 'i believe that india got its first vision of \\n               this in 1857, when we started the war of independence.',\n",
       " 'it is this freedom that\\n               we must protect and nurture and build on.',\n",
       " 'if we are not free, no one will respect us.',\n",
       " 'my second vision for india’s development.',\n",
       " 'for fifty years we have been a developing nation.',\n",
       " 'it is time we see ourselves as a developed nation.',\n",
       " 'we are among the top 5 nations of the world\\n               in terms of gdp.',\n",
       " 'we have a 10 percent growth rate in most areas.',\n",
       " 'our poverty levels are falling.',\n",
       " 'our achievements are being globally recognised today.',\n",
       " 'yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
       " 'isn’t this incorrect?',\n",
       " 'i have a third vision.',\n",
       " 'india must stand up to the world.',\n",
       " 'because i believe that unless india \\n               stands up to the world, no one will respect us.',\n",
       " 'only strength respects strength.',\n",
       " 'we must be \\n               strong not only as a military power but also as an economic power.',\n",
       " 'both must go hand-in-hand.',\n",
       " 'my good fortune was to have worked with three great minds.',\n",
       " 'dr. vikram sarabhai of the dept.',\n",
       " 'of \\n               space, professor satish dhawan, who succeeded him and dr. brahm prakash, father of nuclear material.',\n",
       " 'i was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
       " 'i see four milestones in my career']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "display(corpus)\n",
    "display(sentences)\n",
    "display(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.25883507, 0.30512561,\n",
       "        0.        ],\n",
       "       [0.        , 0.28867513, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings:\n",
    "\n",
    "Word embeddings is the idea of creating mathematical description to make the computer understand analogies like man is to woman as boy is to girl and avoid any biases.\n",
    "How should the  algorithm know that orange and juice or apple and pie are related, orange and apple are similar but juice and pie are not. \n",
    "\n",
    "### Featurised Representation\n",
    "We will add some k number of features against which the words are weighed from -1 to 1 to understand what feature does a particular word may represent. \n",
    "Following are 4 features which are used to represent the 3 words as featurised vectors.\n",
    "\n",
    "........ Man Queen  Orange\n",
    " \n",
    "Gender   =    1           -0.99          0\n",
    "\n",
    "Royal    =    0.02        1             0\n",
    "\n",
    "Colour   =     0         0             0.95\n",
    " \n",
    "Fruit    =      0         0            0.99\n",
    "\n",
    "Word embeddings are beneficial in analysing enormous vocabulary as well, as this will group the similar kind of words together and preserve semantics as well.\n",
    "\n",
    "_Name Entity Recognition Example_\n",
    "\n",
    "Eg- Sally Johnson is an orange farmer.\n",
    "Our model should be able to identify the name Sally Johnson as a person and not an organisation as a farmer is a person. This is carried out by Bidirectional Recurrent Neural Network.\n",
    "\n",
    "Transfer Learning and Word Embedding steps:\n",
    "1. Learn word embeddings from large text corpus(or use pre-trained embedding).\n",
    "2. Transfer embedding to new task with smaller training set.\n",
    "3. Fine tune the word embeddings with new data.\n",
    "\n",
    "Als0, if you know about dealing with images in a NN, encoding is very much similar to embedding, they might even be used interchangably time to time.\n",
    "\n",
    "**Properties of word embeddings** \n",
    "\n",
    "V[Man]-V[Woman] == V[King]-V[Queen]\n",
    "\n",
    "This will result in a difference of 2 in the gender section for both the pairs. The vectors of similar words correlate in properties and their differences as well which can be used to generate analogies in an n-dimensional space(n=no. of features).\n",
    "\n",
    "\n",
    "Visualise the two as a pair of almost parallel vectors in an n dimensional space. This can be used to understand **Cosine Similarity** as well, if the vectors are parallel then the cosine of angle between them would give 1 !\n",
    "\n",
    "That is-- if we ask the question: \n",
    "\n",
    "V[Man]-V[Woman] == V[King]-V[?]\n",
    "The output is highly likely to be Queen.\n",
    "\n",
    "Rupee:India::Pound:[?]\n",
    "\n",
    "<img src=\"C:/Users/Samriddh/python jupyter/NLP Learn/tds.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Embedding Matrix E**\n",
    "\n",
    "A n_feature x n_vocab_size matrix is created which stores all the words with the weighted features according to the meaning of that word.\n",
    "\n",
    "If we multiply a column vector O_i representing any specific word, to this matrix E, we get a (n_feature,1) e_i vector which would constitute the related features described by this word,that is: E.O=e .\n",
    "\n",
    "In practise since the matrix is still largely sparse, we use inbuilt function of Keras to use embedding which uses specialised and optimal function to carry out the process\n",
    "\n",
    "\n",
    "\n",
    "**Learning Embeddings** \n",
    "\n",
    "Let me take the list of words, \"I want a glass of orange\"  and let's start with the first word I. So there's a one add vector with a one in position V[I] . So this is going to be n_vocab_size(lets say 10k) dimensional vector. And what we're going to do is then have a matrix of parameters E, and take E times O to get an embedding vector e[I] , and this step really means that e[I] is obtained by the matrix E times the one hot vector. And then we'll do the same for all of the other words. So now you have a bunch of three dimensional embedding, so each of this is a n_feature(lets say 300) dimensional embedding vector. And what we can do, is fill all of them into a neural network. And then this neural network feeds to a softmax, which has it's own parameters as well. And a softmax classifies among the 10,000 possible outputs in the vocab for those final word we're trying to predict. And so, if in the training slide we saw the word juice then, the target for the softmax in training repeat that it should predict the other word juice was what came after this. So this hidden name here will have his own parameters.I'm going to call this W1 and there's also B1. The softmax there was with own parameters W2, B2, and they're using 300 dimensional word embeddings, then here we have six words. So, this would be six times 300. So this layer or this input will be a 1,800 dimensional vector obtained by taking your six embedding vectors and stacking them together. Well, what's actually more commonly done is to have a fixed historical window. So for example, you might decide that you always want to predict the next word given say the previous four words, where four here is a hyperparameter of the algorithm. So this is how you adjust to either very long or very short sentences or you decide to always just look at the previous four words, so you say, I will still use those four words. And so, let's just get rid of these. And so, if you're always using a four word history, this means that your neural network will input a 1,200 dimensional feature vector, go into this layer, then have a softmax and try to predict the output. And again, variety of choices. And using a fixed history, just means that you can deal with even arbitrarily long sentences because the input sizes are always fixed. And it turns out that this algorithm we'll learn pretty decent word embeddings. And the reason is, if you remember our orange juice, apple juice example, is in the algorithm's incentive to learn pretty similar word embeddings for orange and apple because doing so allows it to fit the training set better because it's going to see orange juice sometimes, or see apple juice sometimes, and so, if you have only a 300 dimensional feature vector to represent all of these words, the algorithm will find that it fits the training set fast.\n",
    "\n",
    "_Generalisation:_ \n",
    "\n",
    "We can set the parameters to learn the context around which the prediction is to be made for example- use first 4 words or last 4 words or right 4 and left 4 words to predict a single word or a sentence as well.\n",
    "\n",
    "Credits: AndrewNG Lec- 5.2.1\n",
    "\n",
    "**Benefits**\n",
    "1. Denser matrix which is way more compact and stores semantics, relations, visualisable model.\n",
    "2. Lower dimension to speed up the math, lesser sparsenes. \n",
    "\n",
    "#### 1. Word2Vec \n",
    "\n",
    "Word2Vec is basically the simplest model of word embedding that has just been discussed.\n",
    "We provide here two examples where word2vec is implemented:\n",
    "\n",
    "**Skip_Gram Model and CBoW Model **\n",
    "\n",
    "\n",
    "We use a context of a sentence and then use that informatino to predict further any particular word/group of worrds by skipping some number of words, hence the neame skip_gram.\n",
    "Let us take an example:\n",
    "I want a glass of orange juice to go along with my cereal.\n",
    "We decide a target word, and a context in which we map the context with the target.\n",
    "Step 1. O_c-->E=e_c\n",
    "Step 2. Softmax(e_c)-->y_hat\n",
    "\n",
    "\n",
    "Softmax:\n",
    "p(t/c) = exp[f(t)'e_c]/Sigma(1,n)exp[f(t)'e_c]\n",
    "\n",
    "We have a loss function and a normalised softmax function\n",
    "\n",
    "This skipgram model is extremely slow in implementation, we can use a heirarchial distribution of classes to manipulate where to look for a particular word instead of searching through the whole model vocabulary.\n",
    "\n",
    "How to sample the context C?\n",
    "\n",
    "1. Removal of stopwords.\n",
    "2. Balance out the less common words from the more common one.\n",
    "3. **Negative Sampling:**\n",
    "\n",
    "Effecient version of skip gram:\n",
    "Problem: Given a pair of words, like orange and juice, we are gonna predict is this a context- target pair?\n",
    "\n",
    "x = Context-target pair\n",
    "y = labelled vector\n",
    "**Orange** -Juice = 1\n",
    "Orange-King = 0  \n",
    "Orange-book = 0\n",
    "Orange-of = 0\n",
    "\n",
    "\n",
    "This is how the dataset will be trained- that is we pick up a context word like orange and pick up k random words from the dictionary and they will be labeled if they are related or not. The k words are chosen on the basis empirical frequency of the words or to use normalised frequency^0.75.\n",
    "\n",
    "The benefit is that instead of a softmax-ed problem we developed a binary problem in this case.\n",
    "\n",
    "**Difference between SkipGram and CBow**\n",
    "The CBOW model learns to predict a target word leveraging all words in its neighborhood. The sum of the context vectors are used to predict the target word. The neighboring words taken into consideration is determined by a pre-defined window size surrounding the target word.\n",
    "\n",
    "The SkipGram model on the other hand, learnt to predict a word based on a neighboring word. To put it simply, given a word, it learns to predict another word in it’s context.\n",
    "\n",
    "\n",
    "[Research paper to refer]https://www.aclweb.org/anthology/D17-1056.pdf\n",
    "[Difference between above two models]https://stackoverflow.com/questions/38287772/cbow-v-s-skip-gram-why-invert-context-and-target-words\n",
    "\n",
    "\n",
    "\n",
    "#### 2. GloVe- Global Vectors for word representations\n",
    "Not used a lot, but it is very simple and works well.\n",
    "\n",
    "Previously we were sampling words based on context and tarfets, but now we wish to globalise this explicitly.\n",
    "\n",
    "\n",
    "X_ij = # times i appears in context of j\n",
    "\n",
    "It might happen that X_ij== X_ji, this means a symmertic semantic similarity, which is useful for forming strong correlations.\n",
    "\n",
    "Model:\n",
    "Minimise this-\n",
    "\n",
    "f(X_ij)(F(t)'e_i+b_i+b_j'-logX_ij)^2       (skip if X_ij=0)\n",
    "\n",
    "f(X_ij)=weighting factor for the target word.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Word2Vec simple one liner code\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "words = model.wv.vocab\n",
    "vector = model.wv['war']\n",
    "similar = model.wv.most_similar('vikram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/66/04faeedb98bfa5f241d0399d0102456886179cabac0355475f23a2978847/gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\samriddh\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\samriddh\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/18/9c/a16951b5a66c86f0ea8ff5aca8d5c700138e708a76412ee7a2ec7fbd4b44/smart_open-4.1.0.tar.gz (116kB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\samriddh\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n",
      "Collecting Cython==0.29.14 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/be/b14be5c3ad1ff73096b518be1538282f053ec34faaca60a8753d975d7e93/Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7MB)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-4.1.0-cp37-none-any.whl size=106210 sha256=a648b4ac6eeccf2c2c42e67cafa5d93929c0bb44cfaa4224d1003aa7c7772403\n",
      "  Stored in directory: C:\\Users\\Samriddh\\AppData\\Local\\pip\\Cache\\wheels\\eb\\83\\5c\\ead33ff91d363db5c2527b563746ba23887669c0221bd2484f\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3 smart-open-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\"\n",
    "\n",
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "\n",
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    \n",
    "    \n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "\n",
    "words = model.wv.vocab\n",
    "\n",
    "# Finding Word Vectors\n",
    "vector = model.wv['war']\n",
    "\n",
    "# Most similar words\n",
    "similar = model.wv.most_similar('freedom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('three', 0.2606293559074402),\n",
       " ('dutch', 0.19812467694282532),\n",
       " ('first', 0.19498443603515625),\n",
       " ('moguls', 0.18788421154022217),\n",
       " ('nuclear', 0.1849081963300705),\n",
       " ('invaded', 0.18165282905101776),\n",
       " ('others.that', 0.17736905813217163),\n",
       " ('people', 0.16865241527557373),\n",
       " ('life', 0.1643061339855194),\n",
       " ('dept', 0.16008752584457397)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar\n",
    "#to see the list of correlated words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and NLP\n",
    "\n",
    "Since I have studied Sequence models from Andrew NG and did the assignments as well. I will upload the DL application notebook separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
